{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Attention Mechanism Best Model Train Test","metadata":{}},{"cell_type":"markdown","source":"**Submitted By:**\nJoyojyoti Acharya - CS21M024,\nVrushab Karia - CS21M075","metadata":{}},{"cell_type":"markdown","source":"### Importing the Necessary Packages","metadata":{}},{"cell_type":"code","source":"#using tensorflow 1.13.2\n!pip install tensorflow==1.13.2\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom tensorflow import keras\ntf.test.gpu_device_name()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:46:28.648186Z","iopub.execute_input":"2022-05-08T09:46:28.648531Z","iopub.status.idle":"2022-05-08T09:47:14.059614Z","shell.execute_reply.started":"2022-05-08T09:46:28.648439Z","shell.execute_reply":"2022-05-08T09:47:14.058896Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Importing Dataset","metadata":{}},{"cell_type":"code","source":"#Using target Language as Hindi\ntarget_language = \"hi\"\nDATAPATH = \"/kaggle/input/dakshina/dakshina_dataset_v1.0/{}/lexicons/{}.translit.sampled.{}.tsv\"\n\n#Defining training, validation and test path and reading the data from dataset.\n\n#Training\ntrain_path = DATAPATH.format(target_language, target_language, \"train\")\ntrain_data = pd.read_csv(train_path, sep = '\\t', header = None)\n\n#Validation\ndev_path = DATAPATH.format(target_language, target_language, \"dev\")\ndev_data = pd.read_csv(dev_path, sep = '\\t', header = None)\n\n#Test\ntest_path = DATAPATH.format(target_language, target_language, \"test\")\ntest_data = pd.read_csv(test_path, sep = '\\t', header = None)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:47:43.420814Z","iopub.execute_input":"2022-05-08T09:47:43.421782Z","iopub.status.idle":"2022-05-08T09:47:43.520612Z","shell.execute_reply.started":"2022-05-08T09:47:43.421737Z","shell.execute_reply":"2022-05-08T09:47:43.519948Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Spliting the dataset into wordwise and characterwise","metadata":{}},{"cell_type":"code","source":"#All unique characters\ninput_characters = set()\ntarget_characters = set()\ninput_characters.add(' ')\ntarget_characters.add(' ')\n\n#Training Data\ntrain_input = [str(w) for w in train_data[1]]\ntrain_target = [\"\\t\" + str(w) + \"\\n\" for w in train_data[0]]\nfor word in train_input:\n    for char in word:\n        input_characters.add(char)\nfor word in train_target:\n    for char in word:\n        target_characters.add(char)\n\n#Validation Data\ndev_input = [str(w) for w in dev_data[1]]\ndev_target = [\"\\t\" + str(w) + \"\\n\" for w in dev_data[0]]\nfor word in dev_input:\n    for char in word:\n        input_characters.add(char)\nfor word in dev_target:\n    for char in word:\n        target_characters.add(char)\n\n#Test Data\ntest_input = [str(w) for w in test_data[1]]\ntest_target = [\"\\t\" + str(w) + \"\\n\" for w in test_data[0]]\n\nfor word in test_input:\n    for char in word:\n        input_characters.add(char) \nfor word in test_target:\n    for char in word:\n        target_characters.add(char)\n        \n#Sorting the characters\ninput_characters = list(input_characters)\ntarget_characters = list(target_characters)\ninput_characters.sort()\ntarget_characters.sort()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:47:44.290842Z","iopub.execute_input":"2022-05-08T09:47:44.292075Z","iopub.status.idle":"2022-05-08T09:47:44.517412Z","shell.execute_reply.started":"2022-05-08T09:47:44.291287Z","shell.execute_reply":"2022-05-08T09:47:44.516633Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Fetching character and maximum sequence length","metadata":{}},{"cell_type":"code","source":"num_encoder_tokens = len(input_characters)\nnum_decoder_tokens = len(target_characters)\nmax_encoder_seq_length = max(max([len(text) for text in train_input]),max([len(text) for text in dev_input]))\nmax_encoder_seq_length = max(max_encoder_seq_length,max([len(text) for text in test_input]))\n                             \nmax_decoder_seq_length = max(max([len(text) for text in train_target]),max([len(text) for text in dev_target]))\nmax_decoder_seq_length = max(max_decoder_seq_length,max([len(text) for text in test_target]))\n                             \nprint(\"Number of Training samples:\", len(train_input))\nprint(\"Number of Validation samples:\", len(dev_input))\nprint(\"Number of Test samples:\", len(test_input))\n                             \nprint(\"Number of unique input tokens:\", num_encoder_tokens)\nprint(\"Number of unique output tokens:\", num_decoder_tokens)\nprint(\"Max sequence length for inputs:\", max_encoder_seq_length)\nprint(\"Max sequence length for outputs:\", max_decoder_seq_length)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:47:44.627828Z","iopub.execute_input":"2022-05-08T09:47:44.628638Z","iopub.status.idle":"2022-05-08T09:47:44.652029Z","shell.execute_reply.started":"2022-05-08T09:47:44.628590Z","shell.execute_reply":"2022-05-08T09:47:44.651016Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Dictionary Indexing and Inverse Dictionary Indexing for the unique Characters","metadata":{}},{"cell_type":"code","source":"input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\ninverse_input_token_index = dict([(i, char) for i, char in enumerate(input_characters)])\ntarget_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\ninverse_target_token_index = dict([(i, char) for i, char in enumerate(target_characters)])","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:47:45.994241Z","iopub.execute_input":"2022-05-08T09:47:45.994542Z","iopub.status.idle":"2022-05-08T09:47:46.000405Z","shell.execute_reply.started":"2022-05-08T09:47:45.994510Z","shell.execute_reply":"2022-05-08T09:47:45.999518Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Training Encoder-Decoder One Hot Data Preparation","metadata":{}},{"cell_type":"code","source":"train_encoder_input_data = np.zeros((len(train_input), max_encoder_seq_length), dtype=\"float32\")\ntrain_decoder_input_data = np.zeros((len(train_input), max_decoder_seq_length), dtype=\"float32\")\ntrain_decoder_target_data = np.zeros((len(train_input), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\nfor i, (input_text, target_text) in enumerate(zip(train_input, train_target)):\n    for t, char in enumerate(input_text):\n        train_encoder_input_data[i, t] = input_token_index[char]\n    train_encoder_input_data[i, t + 1 :] = input_token_index[' ']\n    for t, char in enumerate(target_text):\n        train_decoder_input_data[i, t] = target_token_index[char]\n        if t > 0:\n            train_decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n    train_decoder_input_data[i, t + 1 :] = target_token_index[' ']\n    train_decoder_target_data[i, t:, target_token_index[' ']] =  1.0","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:47:47.571166Z","iopub.execute_input":"2022-05-08T09:47:47.571802Z","iopub.status.idle":"2022-05-08T09:47:48.165219Z","shell.execute_reply.started":"2022-05-08T09:47:47.571759Z","shell.execute_reply":"2022-05-08T09:47:48.164433Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Validation Encoder-Decoder One Hot Data Preparation","metadata":{}},{"cell_type":"code","source":"dev_encoder_input_data = np.zeros((len(dev_input), max_encoder_seq_length), dtype=\"float32\")\ndev_decoder_input_data = np.zeros((len(dev_input), max_decoder_seq_length), dtype=\"float32\")\ndev_decoder_target_data = np.zeros((len(dev_input), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\nfor i, (input_text, target_text) in enumerate(zip(dev_input, dev_target)):\n    for t, char in enumerate(input_text):\n        dev_encoder_input_data[i, t] = input_token_index[char]\n    dev_encoder_input_data[i, t + 1 :] = input_token_index[' ']\n    for t, char in enumerate(target_text):\n        dev_decoder_input_data[i, t] = target_token_index[char]\n        if t > 0:\n            dev_decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n    dev_decoder_input_data[i, t + 1 :] = target_token_index[' ']\n    dev_decoder_target_data[i, t:, target_token_index[' '] ] = 1.0","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:47:48.964559Z","iopub.execute_input":"2022-05-08T09:47:48.965367Z","iopub.status.idle":"2022-05-08T09:47:49.031126Z","shell.execute_reply.started":"2022-05-08T09:47:48.965322Z","shell.execute_reply":"2022-05-08T09:47:49.030154Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### TEST DATA SETUP","metadata":{}},{"cell_type":"code","source":"test_encoder_input_data = np.zeros((len(test_input), max_encoder_seq_length), dtype=\"float32\")\nfor i, input_word in enumerate(test_input):\n    for t, char in enumerate(input_word):\n        test_encoder_input_data[i, t] = input_token_index[char]\n    test_encoder_input_data[i, t + 1 :] = input_token_index[' ']","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:47:50.513360Z","iopub.execute_input":"2022-05-08T09:47:50.513649Z","iopub.status.idle":"2022-05-08T09:47:50.532669Z","shell.execute_reply.started":"2022-05-08T09:47:50.513616Z","shell.execute_reply":"2022-05-08T09:47:50.532011Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### Defining Attention Class","metadata":{}},{"cell_type":"code","source":"#importing packages\nimport tensorflow as tf\nfrom tensorflow.python.keras.layers import Layer\nfrom tensorflow.python.keras import backend as K\n\n#AttentionLayer Class\nclass AttentionLayer(Layer):\n    def __init__(self, **args):\n        super(AttentionLayer, self).__init__(**args)\n    \n    #build function\n    def build(self, input_shape):\n        \n        #random initialization of w_a\n        self.W_a = self.add_weight(name='W_a',\n                                   shape = tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n                                   initializer = 'uniform',\n                                   trainable = True)\n\n        #random initialization of u_a\n        self.U_a = self.add_weight(name = 'U_a',\n                                   shape = tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n                                   initializer = 'uniform',\n                                   trainable = True)\n\n        #random initialization of v_a\n        self.V_a = self.add_weight(name = 'V_a',\n                                   shape = tf.TensorShape((input_shape[0][2], 1)),\n                                   initializer = 'uniform',\n                                   trainable = True)\n\n        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n\n    #call function\n    def call(self, inputs):\n       \n        \"\"\"\n        inputs: [encoder_output_sequence, decoder_output_sequence]\n        \"\"\"\n        encoder_out_seq, decoder_out_seq = inputs\n        \n        #energy_step function\n        def energy_step(inputs, states):\n           \n            \"\"\" Step function for computing energy for a single decoder state\n            inputs: (batchsize * 1 * de_in_dim)\n            states: (batchsize * 1 * de_latent_dim)\n            \"\"\"\n\n            \"\"\" Some parameters required for shaping tensors\"\"\"\n            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n            de_hidden = inputs.shape[-1]\n\n            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n\n            \"\"\" Computing hj.Ua \"\"\"\n            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)\n\n            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n\n            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n            e_i = K.softmax(e_i)\n            \n            return e_i, [e_i]\n\n        #context_step function\n        def context_step(inputs, states):\n            \"\"\" Step function for computing ci using ei \"\"\"\n\n            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n            return c_i, [c_i]\n\n        fake_state_c = K.sum(encoder_out_seq, axis=1)\n        fake_state_e = K.sum(encoder_out_seq, axis=2) \n\n        \"\"\" Computing energy outputs \"\"\"\n        last_out, e_outputs, _ = K.rnn(energy_step, decoder_out_seq, [fake_state_e],)\n\n        \"\"\" Computing context vectors \"\"\"\n        last_out, c_outputs, _ = K.rnn(context_step, e_outputs, [fake_state_c],)\n\n        return c_outputs, e_outputs","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:47:51.769799Z","iopub.execute_input":"2022-05-08T09:47:51.770622Z","iopub.status.idle":"2022-05-08T09:47:51.787321Z","shell.execute_reply.started":"2022-05-08T09:47:51.770577Z","shell.execute_reply":"2022-05-08T09:47:51.786489Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### Wordwise Inference Mechanism for Attention Approach","metadata":{}},{"cell_type":"code","source":"#importing packages\nimport numpy as np\nfrom tensorflow import keras\nfrom random import sample\n\n#sigmoid function\ndef sigmoid(i):\n    return [1/(1 + np.exp(-z)) for z in i]\n\n# Attention_Inference Function\ndef attention_inference(model, dev_encoder_input_data, test_input, test_target, num_decoder_tokens, max_decoder_seq_length, target_token_index, inverse_target_token_index, latent_dim, model_name):\n    \n    #Function for Sequence Prediction\n    def decode_sequence_prediction(input_sequence):\n        # Encode the input as state vectors.\n        encoder_outputs = encoder_model.predict(input_sequence)\n        encoder_output, states_value = encoder_outputs[0], encoder_outputs[1:]\n        \n        # Generate empty target sequence of length 1.\n        target_sequence = np.zeros((1, 1))\n\n        # Populate the first character of target sequence with the start character.\n        target_sequence[0, 0] = target_token_index[\"\\t\"]\n        \n        flag = True\n        output_sequence = \"\"\n\n        while flag:\n            output = decoder_model.predict([target_sequence] + states_value + [encoder_output])\n            output_tokens, states_value, attention_weights = output[0], output[1:-1], output[-1]\n\n            # Sample a token/character\n            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n            sampled_character = inverse_target_token_index[sampled_token_index]\n            output_sequence += sampled_character\n\n            if sampled_character == \"\\n\" or len(output_sequence) > max_decoder_seq_length:\n                flag = False\n\n            target_sequence = np.zeros((1, 1))\n            target_sequence[0, 0] = sampled_token_index\n\n        return output_sequence\n    \n    print(model.summary())\n\n    # Encoder Model\n    encoder_inputs = model.input[0]\n\n    if model_name == \"RNN\" or model_name == \"GRU\":\n        encoder_outputs, state = model.layers[4].output\n        encoder_model = keras.Model(encoder_inputs, [encoder_outputs] + [state])\n    \n    elif model_name == \"LSTM\":\n        encoder_outputs, state_h_enc, state_c_enc = model.layers[4].output\n        encoder_model = keras.Model(encoder_inputs, [encoder_outputs] + [state_h_enc, state_c_enc])\n    \n    else:\n        print(\"Wrong Choice of Model\")\n        return\n\n    #Decoder Model\n    decoder_inputs = model.input[1]  # input_2\n    decoder_outputs = model.layers[3](decoder_inputs)\n\n    if model_name == \"RNN\" or model_name == \"GRU\":\n        state = keras.Input(shape = (latent_dim, ))\n        decoder_states_inputs = [state]\n        decoder_outputs, state = model.layers[5](decoder_outputs, initial_state = decoder_states_inputs)\n        decoder_states = [state]\n\n    elif model_name == \"LSTM\":\n        state_h_dec, state_c_dec = keras.Input(shape = (latent_dim, )), keras.Input(shape = (latent_dim, ))\n        decoder_states_inputs = [state_h_dec, state_c_dec]\n        decoder_outputs, state_h_dec, state_c_dec = model.layers[5](decoder_outputs, initial_state = decoder_states_inputs)\n        decoder_states = [state_h_dec, state_c_dec]\n        \n    else:\n        print(\"Wrong Choice of Model\")\n        \n    attention_inputs = keras.Input(shape = (None, latent_dim, ))\n    attention_output, attention_scores = model.layers[6]([attention_inputs, decoder_outputs])\n    concatenated_decoder_input = model.layers[7]([decoder_outputs, attention_output])\n\n    # Decoder Dense layer\n    decoder_dense = model.layers[8]\n    decoder_outputs = decoder_dense(concatenated_decoder_input)\n\n    # Final decoder model\n    decoder_model = keras.Model([decoder_inputs] + decoder_states_inputs + [attention_inputs], [decoder_outputs] + decoder_states + [attention_scores])\n\n    #count the correct predictions\n    correct_count, test_size = 0, len(test_input)\n    \n    #File to Laod the Prediction\n    attention_prediction = open(\"/kaggle/working/predictions_attention.csv\", \"w\", encoding='utf-8')\n    attention_prediction.write(\"Input Sentence,Original Target Sentence,Predicted Output Sentence\\n\")\n    for i in range(test_size):\n        # Take one sequence (part of the training set)\n        if i%50==0:\n            print(\"Testing at: \",i)\n        input_sequence = dev_encoder_input_data[i : i + 1]\n        decoded_word = decode_sequence_prediction(input_sequence)\n        original_word = test_target[i][1:]\n        attention_prediction.write(test_input[i] + \",\" + decoded_word[:-1] + \",\" + original_word[:-1] + \"\\n\")\n        if(original_word == decoded_word):\n            correct_count += 1\n            \n    return correct_count / test_size\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:47:53.389456Z","iopub.execute_input":"2022-05-08T09:47:53.389742Z","iopub.status.idle":"2022-05-08T09:47:53.415205Z","shell.execute_reply.started":"2022-05-08T09:47:53.389711Z","shell.execute_reply":"2022-05-08T09:47:53.414337Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### Main Block to Train the Best Model","metadata":{}},{"cell_type":"code","source":"#importing packages \nimport numpy as np\nfrom tensorflow import keras\n\n#parameters in the main block to train the model\nhidden_layer_size=384\nlearning_rate=0.001\noptimizer='adam'\nbatch_size=512\nmodel_name = \"LSTM\"\nembedding_size = 512\ndropout = 0.3\nepochs = 20\n        \n#Encoder Model\nencoder_inputs = keras.Input(shape = (None, ))\nencoder_outputs = keras.layers.Embedding(input_dim = num_encoder_tokens, output_dim = embedding_size, input_length = max_encoder_seq_length)(encoder_inputs)\n\n# Encoder Model Choice\nif model_name == \"RNN\":\n    encoder_outputs, state = keras.layers.SimpleRNN(hidden_layer_size, dropout = dropout, return_state = True, return_sequences = True)(encoder_outputs)\n    encoder_states = [state]\nelif model_name == \"LSTM\":\n    encoder_outputs, state_h, state_c = keras.layers.LSTM(hidden_layer_size, dropout = dropout, return_state = True, return_sequences = True)(encoder_outputs)\n    encoder_states = [state_h,state_c]\nelif model_name == \"GRU\":\n    encoder_outputs, state = keras.layers.GRU(hidden_layer_size, dropout = dropout, return_state = True, return_sequences = True)(encoder_outputs)\n    encoder_states = [state]\nelse:\n    print(\"Wrong Choice of Model\")\n\n# Decoder Model\ndecoder_inputs = keras.Input(shape=(None, ))\ndecoder_outputs = keras.layers.Embedding(input_dim = num_decoder_tokens, output_dim = embedding_size, input_length = max_decoder_seq_length)(decoder_inputs)\n\n# We will test on only one layer of encoder and only one layer of decoder model\n\nif model_name == \"RNN\":\n    decoder = keras.layers.SimpleRNN(hidden_layer_size, dropout = dropout, return_sequences = True, return_state = True)\n    decoder_outputs, state = decoder(decoder_outputs, initial_state = encoder_states)\n    decoder_states = [state]\nelif model_name == \"LSTM\":\n    decoder = keras.layers.LSTM(hidden_layer_size, dropout = dropout, return_sequences = True, return_state = True)\n    decoder_outputs, state_h, state_c = decoder(decoder_outputs, initial_state = encoder_states)\n    decoder_states = [state_h, state_c]\nelif model_name == \"GRU\":\n    decoder = keras.layers.GRU(hidden_layer_size, dropout = dropout, return_sequences = True, return_state = True)\n    decoder_outputs, state = decoder(decoder_outputs, initial_state = encoder_states)\n    decoder_states = [state]\nelse:\n    print(\"Wrong Model Choice\")\n        \n# Adding Attention Layer\nattention = AttentionLayer()\nattention_output, _ = attention([encoder_outputs, decoder_outputs])\nconcatenated_decoder_input = keras.layers.Concatenate(axis = -1)([decoder_outputs, attention_output])\n\n#Decoder Dense Layer\ndecoder_dense = keras.layers.Dense(num_decoder_tokens, activation = \"softmax\")\ndecoder_outputs = decoder_dense(concatenated_decoder_input)\n\n#Runnable Model\nmodel = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\nmodel.summary()\n    \n#Different Optimizers\nif optimizer == 'adam':\n    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\nelif optimizer == 'nadam':\n    model.compile(optimizer=\"nadam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\nelif optimizer == 'rmsprop':\n    model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\nelse:\n    print(\"Wrong optimizer Choice...\")\n        \n#Model fitting with train and validation data characterwise\nmodel.fit(\n    [train_encoder_input_data, train_decoder_input_data],\n    train_decoder_target_data,\n    batch_size = batch_size,\n    epochs = epochs,\n    validation_data = ([dev_encoder_input_data, dev_decoder_input_data], dev_decoder_target_data)\n)\n\n#Wordwise Validation Data and Accuracy on the model\nvalidation_accuracy = attention_inference(model,dev_encoder_input_data, dev_input, dev_target, num_decoder_tokens, max_decoder_seq_length, target_token_index, inverse_target_token_index, hidden_layer_size, model_name)\nprint(\"Attention_Wordwise_Val_Accuracy: \", validation_accuracy)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T09:47:54.873173Z","iopub.execute_input":"2022-05-08T09:47:54.873593Z","iopub.status.idle":"2022-05-08T12:47:19.084038Z","shell.execute_reply.started":"2022-05-08T09:47:54.873557Z","shell.execute_reply":"2022-05-08T12:47:19.082067Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"model.save(\"Attention_best_model\")","metadata":{"execution":{"iopub.status.busy":"2022-05-08T12:47:19.091592Z","iopub.execute_input":"2022-05-08T12:47:19.091964Z","iopub.status.idle":"2022-05-08T12:47:20.540453Z","shell.execute_reply.started":"2022-05-08T12:47:19.091890Z","shell.execute_reply":"2022-05-08T12:47:20.539694Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### Test Accuracy","metadata":{}},{"cell_type":"code","source":"#Wordwise Validation Data and Accuracy on the model\ntest_accuracy = attention_inference(model,test_encoder_input_data, test_input, test_target, num_decoder_tokens, max_decoder_seq_length, target_token_index, inverse_target_token_index, hidden_layer_size, model_name)\nprint(\"Attention_Wordwise_Test_Accuracy: \", test_accuracy)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T12:47:20.541495Z","iopub.execute_input":"2022-05-08T12:47:20.543196Z","iopub.status.idle":"2022-05-08T12:50:01.806311Z","shell.execute_reply.started":"2022-05-08T12:47:20.543163Z","shell.execute_reply":"2022-05-08T12:50:01.804309Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## Visualization of HeatMaps","metadata":{}},{"cell_type":"markdown","source":"### Wordwise Attention Inference Model with Heatmaps","metadata":{}},{"cell_type":"code","source":"#importing packages\nimport numpy as np\nfrom tensorflow import keras\nfrom random import sample\n\n\n#sigmoid function\ndef sigmoid(i):\n    return [1/(1 + np.exp(-z)) for z in i]\n\n# Attention_Inference Function\ndef attention_inference_heatmaps(model, dev_encoder_input_data, test_input, test_target, num_decoder_tokens, max_decoder_seq_length, target_token_index, inverse_target_token_index, latent_dim, model_name):\n    \n    #Function for Sequence Prediction\n    def decode_sequence_prediction(input_sequence):\n        # Encode the input as state vectors.\n        encoder_outputs = encoder_model.predict(input_sequence)\n        encoder_output, states_value = encoder_outputs[0], encoder_outputs[1:]\n        \n        # Generate empty target sequence of length 1.\n        target_sequence = np.zeros((1, 1))\n\n        # Populate the first character of target sequence with the start character.\n        target_sequence[0, 0] = target_token_index[\"\\t\"]\n        \n        #Heatmaps and Visualization Data\n        heatmap = []\n        visualization = []\n        flag = True\n        output_sequence = \"\"\n        \n\n        while flag:\n            output = decoder_model.predict([target_sequence] + states_value + [encoder_output])\n            output_tokens, states_value, attention_weights = output[0], output[1:-1], output[-1]\n\n            # Sample a token/character\n            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n            sampled_character = inverse_target_token_index[sampled_token_index]\n            output_sequence += sampled_character\n\n            if sampled_character == \"\\n\" or len(output_sequence) > max_decoder_seq_length:\n                flag = False\n\n            target_sequence = np.zeros((1, 1))\n            target_sequence[0, 0] = sampled_token_index\n            heatmap.append((sampled_character, attention_weights))\n            visualization.append((sampled_character, states_value[0]))\n\n        return output_sequence, heatmap, visualization\n    \n    print(model.summary())\n\n    # Encoder Model\n    encoder_inputs = model.input[0]\n\n    if model_name == \"RNN\" or model_name == \"GRU\":\n        encoder_outputs, state = model.layers[4].output\n        encoder_model = keras.Model(encoder_inputs, [encoder_outputs] + [state])\n    \n    elif model_name == \"LSTM\":\n        encoder_outputs, state_h_enc, state_c_enc = model.layers[4].output\n        encoder_model = keras.Model(encoder_inputs, [encoder_outputs] + [state_h_enc, state_c_enc])\n    \n    else:\n        print(\"Wrong Choice of Model\")\n        return\n\n    #Decoder Model\n    decoder_inputs = model.input[1]  # input_2\n    decoder_outputs = model.layers[3](decoder_inputs)\n\n    if model_name == \"RNN\" or model_name == \"GRU\":\n        state = keras.Input(shape = (latent_dim, ))\n        decoder_states_inputs = [state]\n        decoder_outputs, state = model.layers[5](decoder_outputs, initial_state = decoder_states_inputs)\n        decoder_states = [state]\n\n    elif model_name == \"LSTM\":\n        state_h_dec, state_c_dec = keras.Input(shape = (latent_dim, )), keras.Input(shape = (latent_dim, ))\n        decoder_states_inputs = [state_h_dec, state_c_dec]\n        decoder_outputs, state_h_dec, state_c_dec = model.layers[5](decoder_outputs, initial_state = decoder_states_inputs)\n        decoder_states = [state_h_dec, state_c_dec]\n        \n    else:\n        print(\"Wrong Choice of Model\")\n        \n    attention_inputs = keras.Input(shape = (None, latent_dim, ))\n    attention_output, attention_scores = model.layers[6]([attention_inputs, decoder_outputs])\n    concatenated_decoder_input = model.layers[7]([decoder_outputs, attention_output])\n\n    # Decoder Dense layer\n    decoder_dense = model.layers[8]\n    decoder_outputs = decoder_dense(concatenated_decoder_input)\n\n    # Final decoder model\n    decoder_model = keras.Model([decoder_inputs] + decoder_states_inputs + [attention_inputs], [decoder_outputs] + decoder_states + [attention_scores])\n\n    #count the correct predictions\n    correct_count, test_size = 0, len(test_input)\n    \n    #Heatmaps and Visualizations\n    visualisations = sample(range(test_size), 10)\n    heatmaps = []\n    \n    #File to Laod the Prediction\n    attention_prediction = open(\"/kaggle/working/predictions_attention.csv\", \"w\", encoding='utf-8')\n    attention_prediction.write(\"Input Sentence,Original Target Sentence,Predicted Output Sentence\\n\")\n    for i in range(test_size):\n        # Take one sequence (part of the training set)\n        if i%50==0:\n            print(\"Testing at: \",i)\n        input_sequence = dev_encoder_input_data[i : i + 1]\n        decoded_word, heatmap, visualization = decode_sequence_prediction(input_sequence)\n        original_word = test_target[i][1:]\n        attention_prediction.write(test_input[i] + \",\" + decoded_word[:-1] + \",\" + original_word[:-1] + \"\\n\")\n        if(original_word == decoded_word):\n            correct_count += 1\n            \n        #Heatmaps and Visualizations\n        if i in visualisations:\n            \n            # Connectivity Visualization - Q6\n            with open(\"connectivity_visualization.txt\", \"a\", encoding='utf-8') as file:\n\n                actual_word = test_input[i]\n\n                \"\"\"writing to the output file for connectivity\"\"\"\n                \n                file.write(actual_word)\n                file.write(\"\\t\")\n                file.write(str(len(heatmap)))\n                file.write(\"\\n\")\n\n                for t in range(len(heatmap)):\n                    dec_char = heatmap[t][0]\n                    dec_char_prob = heatmap[t][1].reshape(-1)\n                \n                    if t != len(heatmap) - 1: \n                        file.write(dec_char)\n                    else:\n                        file.write(\"<e>\")\n                    \n                    file.write(\"\\t\")\n\n                    for p in range(len(actual_word)):\n                        file.write(str(dec_char_prob[p]))\n                        file.write(\"\\t\")\n\n                    file.write(\"\\n\")\n\n                file.write(\"Next\\n\")\n            \n            # Heatmap Plot Data\n            heatmaps.append((test_input[i], heatmap))\n            \n    return correct_count / test_size, heatmaps\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T12:51:40.855358Z","iopub.execute_input":"2022-05-08T12:51:40.856008Z","iopub.status.idle":"2022-05-08T12:51:40.886669Z","shell.execute_reply.started":"2022-05-08T12:51:40.855951Z","shell.execute_reply":"2022-05-08T12:51:40.885790Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## Finding Heatmaps","metadata":{}},{"cell_type":"code","source":"#Wordwise Validation Data and Accuracy on the model\ntest_accuracy, heatmaps = attention_inference_heatmaps(model,test_encoder_input_data, test_input, test_target, num_decoder_tokens, max_decoder_seq_length, target_token_index, inverse_target_token_index, hidden_layer_size, model_name)\nprint(\"Attention_Wordwise_Test_Accuracy: \", test_accuracy)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T12:51:44.757315Z","iopub.execute_input":"2022-05-08T12:51:44.757604Z","iopub.status.idle":"2022-05-08T12:54:22.833908Z","shell.execute_reply.started":"2022-05-08T12:51:44.757570Z","shell.execute_reply":"2022-05-08T12:54:22.833005Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### Heatmap Plots Function","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom matplotlib.font_manager import FontProperties\n#HeatMap Plots function\ndef attention_heatmap_plot(input_sequence, heatmap):\n    matrix = []\n    decoder_inputs = []\n\n    for data in heatmap:\n        attn, idx = data[1], data[0], \n        matrix.append(attn.reshape(-1)[:len(input_sequence)])\n        decoder_inputs.append(idx)\n    \n\n    figure, axis = plt.subplots()\n    axis.imshow(np.array(matrix))\n    \n    #We have used \"nirmala.ttf\" for the \"HINDI\" font.\n    axis.set_yticklabels([inp if inp != '\\n' else \"<e>\" for inp in decoder_inputs], fontproperties = FontProperties(fname = \"/kaggle/input/nirmala/nirmala.ttf\"))\n    axis.set_xticklabels([char for char in input_sequence])\n\n    axis.set_xticks(np.arange(np.array(matrix).shape[1]))\n    axis.set_yticks(np.arange(np.array(matrix).shape[0]))\n\n    axis.tick_params(labelsize = 15)\n\n    return figure","metadata":{"execution":{"iopub.status.busy":"2022-05-08T12:54:56.888680Z","iopub.execute_input":"2022-05-08T12:54:56.889363Z","iopub.status.idle":"2022-05-08T12:54:56.898508Z","shell.execute_reply.started":"2022-05-08T12:54:56.889327Z","shell.execute_reply":"2022-05-08T12:54:56.897807Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### Wandb Log of the sample HeatMaps","metadata":{}},{"cell_type":"code","source":"import wandb\nwandb.init(project=\"CS6910-Assignment-3\", entity=\"cs21m024_cs21m075\")\nwandb.log({\"Q5D\": [wandb.Image(attention_heatmap_plot(image[0],image[1])) for image in heatmaps]})\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T12:56:47.194934Z","iopub.execute_input":"2022-05-08T12:56:47.195227Z","iopub.status.idle":"2022-05-08T12:57:18.494509Z","shell.execute_reply.started":"2022-05-08T12:56:47.195193Z","shell.execute_reply":"2022-05-08T12:57:18.493495Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}