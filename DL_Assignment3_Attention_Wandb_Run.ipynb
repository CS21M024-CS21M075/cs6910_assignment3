{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Attention Mechanism Wandb Run Code","metadata":{}},{"cell_type":"markdown","source":"**Submitted By:**\nJoyojyoti Acharya - CS21M024,\nVrushab Karia - CS21M075","metadata":{}},{"cell_type":"markdown","source":"### Importing the Necessary Packages","metadata":{}},{"cell_type":"code","source":"#using tensorflow 1.13.2\n!pip install tensorflow==1.13.2\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom tensorflow import keras\ntf.test.gpu_device_name()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T20:50:06.592431Z","iopub.execute_input":"2022-05-07T20:50:06.592968Z","iopub.status.idle":"2022-05-07T20:50:50.051027Z","shell.execute_reply.started":"2022-05-07T20:50:06.592870Z","shell.execute_reply":"2022-05-07T20:50:50.049989Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Importing Dataset","metadata":{}},{"cell_type":"code","source":"#Using target Language as Hindi\ntarget_language = \"hi\"\nDATAPATH = \"/kaggle/input/dakshina/dakshina_dataset_v1.0/{}/lexicons/{}.translit.sampled.{}.tsv\"\n\n#Defining training, validation and test path and reading the data from dataset.\n\n#Training\ntrain_path = DATAPATH.format(target_language, target_language, \"train\")\ntrain_data = pd.read_csv(train_path, sep = '\\t', header = None)\n\n#Validation\ndev_path = DATAPATH.format(target_language, target_language, \"dev\")\ndev_data = pd.read_csv(dev_path, sep = '\\t', header = None)\n\n#Test\ntest_path = DATAPATH.format(target_language, target_language, \"test\")\ntest_data = pd.read_csv(test_path, sep = '\\t', header = None)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T20:50:50.053147Z","iopub.execute_input":"2022-05-07T20:50:50.053757Z","iopub.status.idle":"2022-05-07T20:50:50.175293Z","shell.execute_reply.started":"2022-05-07T20:50:50.053703Z","shell.execute_reply":"2022-05-07T20:50:50.174613Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Spliting the dataset into wordwise and characterwise","metadata":{}},{"cell_type":"code","source":"#All unique characters\ninput_characters = set()\ntarget_characters = set()\ninput_characters.add(' ')\ntarget_characters.add(' ')\n\n#Training Data\ntrain_input = [str(w) for w in train_data[1]]\ntrain_target = [\"\\t\" + str(w) + \"\\n\" for w in train_data[0]]\nfor word in train_input:\n    for char in word:\n        input_characters.add(char)\nfor word in train_target:\n    for char in word:\n        target_characters.add(char)\n\n#Validation Data\ndev_input = [str(w) for w in dev_data[1]]\ndev_target = [\"\\t\" + str(w) + \"\\n\" for w in dev_data[0]]\nfor word in dev_input:\n    for char in word:\n        input_characters.add(char)\nfor word in dev_target:\n    for char in word:\n        target_characters.add(char)\n\n#Test Data\ntest_input = [str(w) for w in test_data[1]]\ntest_target = [\"\\t\" + str(w) + \"\\n\" for w in test_data[0]]\n\nfor word in test_input:\n    for char in word:\n        input_characters.add(char) \nfor word in test_target:\n    for char in word:\n        target_characters.add(char)\n        \n#Sorting the characters\ninput_characters = list(input_characters)\ntarget_characters = list(target_characters)\ninput_characters.sort()\ntarget_characters.sort()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T20:50:50.176489Z","iopub.execute_input":"2022-05-07T20:50:50.176801Z","iopub.status.idle":"2022-05-07T20:50:51.757092Z","shell.execute_reply.started":"2022-05-07T20:50:50.176770Z","shell.execute_reply":"2022-05-07T20:50:51.756070Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Fetching character and maximum sequence length","metadata":{}},{"cell_type":"code","source":"num_encoder_tokens = len(input_characters)\nnum_decoder_tokens = len(target_characters)\nmax_encoder_seq_length = max(max([len(text) for text in train_input]),max([len(text) for text in dev_input]))\nmax_encoder_seq_length = max(max_encoder_seq_length,max([len(text) for text in test_input]))\n                             \nmax_decoder_seq_length = max(max([len(text) for text in train_target]),max([len(text) for text in dev_target]))\nmax_decoder_seq_length = max(max_decoder_seq_length,max([len(text) for text in test_target]))\n                             \nprint(\"Number of Training samples:\", len(train_input))\nprint(\"Number of Validation samples:\", len(dev_input))\nprint(\"Number of Test samples:\", len(test_input))\n                             \nprint(\"Number of unique input tokens:\", num_encoder_tokens)\nprint(\"Number of unique output tokens:\", num_decoder_tokens)\nprint(\"Max sequence length for inputs:\", max_encoder_seq_length)\nprint(\"Max sequence length for outputs:\", max_decoder_seq_length)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T20:50:51.759802Z","iopub.execute_input":"2022-05-07T20:50:51.760039Z","iopub.status.idle":"2022-05-07T20:50:51.797251Z","shell.execute_reply.started":"2022-05-07T20:50:51.760008Z","shell.execute_reply":"2022-05-07T20:50:51.796144Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Dictionary Indexing and Inverse Dictionary Indexing for the unique Characters","metadata":{}},{"cell_type":"code","source":"input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\ninverse_input_token_index = dict([(i, char) for i, char in enumerate(input_characters)])\ntarget_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\ninverse_target_token_index = dict([(i, char) for i, char in enumerate(target_characters)])","metadata":{"execution":{"iopub.status.busy":"2022-05-07T20:50:51.798670Z","iopub.execute_input":"2022-05-07T20:50:51.798983Z","iopub.status.idle":"2022-05-07T20:50:51.806028Z","shell.execute_reply.started":"2022-05-07T20:50:51.798941Z","shell.execute_reply":"2022-05-07T20:50:51.805255Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Training Encoder-Decoder One Hot Data Preparation","metadata":{}},{"cell_type":"code","source":"train_encoder_input_data = np.zeros((len(train_input), max_encoder_seq_length), dtype=\"float32\")\ntrain_decoder_input_data = np.zeros((len(train_input), max_decoder_seq_length), dtype=\"float32\")\ntrain_decoder_target_data = np.zeros((len(train_input), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\nfor i, (input_text, target_text) in enumerate(zip(train_input, train_target)):\n    for t, char in enumerate(input_text):\n        train_encoder_input_data[i, t] = input_token_index[char]\n    train_encoder_input_data[i, t + 1 :] = input_token_index[' ']\n    for t, char in enumerate(target_text):\n        train_decoder_input_data[i, t] = target_token_index[char]\n        if t > 0:\n            train_decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n    train_decoder_input_data[i, t + 1 :] = target_token_index[' ']\n    train_decoder_target_data[i, t:, target_token_index[' ']] =  1.0","metadata":{"execution":{"iopub.status.busy":"2022-05-07T20:50:51.807446Z","iopub.execute_input":"2022-05-07T20:50:51.807704Z","iopub.status.idle":"2022-05-07T20:50:52.738976Z","shell.execute_reply.started":"2022-05-07T20:50:51.807673Z","shell.execute_reply":"2022-05-07T20:50:52.738332Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Validation Encoder-Decoder One Hot Data Preparation","metadata":{}},{"cell_type":"code","source":"dev_encoder_input_data = np.zeros((len(dev_input), max_encoder_seq_length), dtype=\"float32\")\ndev_decoder_input_data = np.zeros((len(dev_input), max_decoder_seq_length), dtype=\"float32\")\ndev_decoder_target_data = np.zeros((len(dev_input), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\nfor i, (input_text, target_text) in enumerate(zip(dev_input, dev_target)):\n    for t, char in enumerate(input_text):\n        dev_encoder_input_data[i, t] = input_token_index[char]\n    dev_encoder_input_data[i, t + 1 :] = input_token_index[' ']\n    for t, char in enumerate(target_text):\n        dev_decoder_input_data[i, t] = target_token_index[char]\n        if t > 0:\n            dev_decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n    dev_decoder_input_data[i, t + 1 :] = target_token_index[' ']\n    dev_decoder_target_data[i, t:, target_token_index[' '] ] = 1.0","metadata":{"execution":{"iopub.status.busy":"2022-05-07T20:50:52.740357Z","iopub.execute_input":"2022-05-07T20:50:52.740752Z","iopub.status.idle":"2022-05-07T20:50:52.838511Z","shell.execute_reply.started":"2022-05-07T20:50:52.740719Z","shell.execute_reply":"2022-05-07T20:50:52.837653Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Defining Attention Class","metadata":{}},{"cell_type":"code","source":"#importing packages\nimport tensorflow as tf\nfrom tensorflow.python.keras.layers import Layer\nfrom tensorflow.python.keras import backend as K\n\n#AttentionLayer Class\nclass AttentionLayer(Layer):\n    def __init__(self, **args):\n        super(AttentionLayer, self).__init__(**args)\n    \n    #build function\n    def build(self, input_shape):\n        \n        #random initialization of w_a\n        self.W_a = self.add_weight(name='W_a',\n                                   shape = tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n                                   initializer = 'uniform',\n                                   trainable = True)\n\n        #random initialization of u_a\n        self.U_a = self.add_weight(name = 'U_a',\n                                   shape = tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n                                   initializer = 'uniform',\n                                   trainable = True)\n\n        #random initialization of v_a\n        self.V_a = self.add_weight(name = 'V_a',\n                                   shape = tf.TensorShape((input_shape[0][2], 1)),\n                                   initializer = 'uniform',\n                                   trainable = True)\n\n        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n\n    #call function\n    def call(self, inputs):\n       \n        \"\"\"\n        inputs: [encoder_output_sequence, decoder_output_sequence]\n        \"\"\"\n        encoder_out_seq, decoder_out_seq = inputs\n        \n        #energy_step function\n        def energy_step(inputs, states):\n           \n            \"\"\" Step function for computing energy for a single decoder state\n            inputs: (batchsize * 1 * de_in_dim)\n            states: (batchsize * 1 * de_latent_dim)\n            \"\"\"\n\n            \"\"\" Some parameters required for shaping tensors\"\"\"\n            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n            de_hidden = inputs.shape[-1]\n\n            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n\n            \"\"\" Computing hj.Ua \"\"\"\n            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)\n\n            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n\n            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n            e_i = K.softmax(e_i)\n            \n            return e_i, [e_i]\n\n        #context_step function\n        def context_step(inputs, states):\n            \"\"\" Step function for computing ci using ei \"\"\"\n\n            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n            return c_i, [c_i]\n\n        fake_state_c = K.sum(encoder_out_seq, axis=1)\n        fake_state_e = K.sum(encoder_out_seq, axis=2) \n\n        \"\"\" Computing energy outputs \"\"\"\n        last_out, e_outputs, _ = K.rnn(energy_step, decoder_out_seq, [fake_state_e],)\n\n        \"\"\" Computing context vectors \"\"\"\n        last_out, c_outputs, _ = K.rnn(context_step, e_outputs, [fake_state_c],)\n\n        return c_outputs, e_outputs","metadata":{"execution":{"iopub.status.busy":"2022-05-07T20:50:52.839896Z","iopub.execute_input":"2022-05-07T20:50:52.840145Z","iopub.status.idle":"2022-05-07T20:50:52.868257Z","shell.execute_reply.started":"2022-05-07T20:50:52.840113Z","shell.execute_reply":"2022-05-07T20:50:52.867338Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Wordwise Inference Mechanism for Attention Approach","metadata":{}},{"cell_type":"code","source":"#importing packages\nimport numpy as np\nfrom tensorflow import keras\nfrom random import sample\n\n#sigmoid function\ndef sigmoid(i):\n    return [1/(1 + np.exp(-z)) for z in i]\n\n# Attention_Inference Function\ndef attention_inference(model, dev_encoder_input_data, test_input, test_target, num_decoder_tokens, max_decoder_seq_length, target_token_index, inverse_target_token_index, latent_dim, model_name):\n    \n    #Function for Sequence Prediction\n    def decode_sequence_prediction(input_sequence):\n        # Encode the input as state vectors.\n        encoder_outputs = encoder_model.predict(input_sequence)\n        encoder_output, states_value = encoder_outputs[0], encoder_outputs[1:]\n        \n        # Generate empty target sequence of length 1.\n        target_sequence = np.zeros((1, 1))\n\n        # Populate the first character of target sequence with the start character.\n        target_sequence[0, 0] = target_token_index[\"\\t\"]\n        \n        flag = True\n        output_sequence = \"\"\n\n        while flag:\n            output = decoder_model.predict([target_sequence] + states_value + [encoder_output])\n            output_tokens, states_value, attention_weights = output[0], output[1:-1], output[-1]\n\n            # Sample a token/character\n            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n            sampled_character = inverse_target_token_index[sampled_token_index]\n            output_sequence += sampled_character\n\n            if sampled_character == \"\\n\" or len(output_sequence) > max_decoder_seq_length:\n                flag = False\n\n            target_sequence = np.zeros((1, 1))\n            target_sequence[0, 0] = sampled_token_index\n\n        return output_sequence\n    \n    print(model.summary())\n\n    # Encoder Model\n    encoder_inputs = model.input[0]\n\n    if model_name == \"RNN\" or model_name == \"GRU\":\n        encoder_outputs, state = model.layers[4].output\n        encoder_model = keras.Model(encoder_inputs, [encoder_outputs] + [state])\n    \n    elif model_name == \"LSTM\":\n        encoder_outputs, state_h_enc, state_c_enc = model.layers[4].output\n        encoder_model = keras.Model(encoder_inputs, [encoder_outputs] + [state_h_enc, state_c_enc])\n    \n    else:\n        print(\"Wrong Choice of Model\")\n        return\n\n    #Decoder Model\n    decoder_inputs = model.input[1]  # input_2\n    decoder_outputs = model.layers[3](decoder_inputs)\n\n    if model_name == \"RNN\" or model_name == \"GRU\":\n        state = keras.Input(shape = (latent_dim, ))\n        decoder_states_inputs = [state]\n        decoder_outputs, state = model.layers[5](decoder_outputs, initial_state = decoder_states_inputs)\n        decoder_states = [state]\n\n    elif model_name == \"LSTM\":\n        state_h_dec, state_c_dec = keras.Input(shape = (latent_dim, )), keras.Input(shape = (latent_dim, ))\n        decoder_states_inputs = [state_h_dec, state_c_dec]\n        decoder_outputs, state_h_dec, state_c_dec = model.layers[5](decoder_outputs, initial_state = decoder_states_inputs)\n        decoder_states = [state_h_dec, state_c_dec]\n        \n    else:\n        print(\"Wrong Choice of Model\")\n        \n    attention_inputs = keras.Input(shape = (None, latent_dim, ))\n    attention_output, attention_scores = model.layers[6]([attention_inputs, decoder_outputs])\n    concatenated_decoder_input = model.layers[7]([decoder_outputs, attention_output])\n\n    # Decoder Dense layer\n    decoder_dense = model.layers[8]\n    decoder_outputs = decoder_dense(concatenated_decoder_input)\n\n    # Final decoder model\n    decoder_model = keras.Model([decoder_inputs] + decoder_states_inputs + [attention_inputs], [decoder_outputs] + decoder_states + [attention_scores])\n\n    #count the correct predictions\n    correct_count, test_size = 0, len(test_input)\n    for i in range(test_size):\n        # Take one sequence (part of the training set)\n        if i%50==0:\n            print(\"Testing at: \",i)\n        input_sequence = dev_encoder_input_data[i : i + 1]\n        decoded_word = decode_sequence_prediction(input_sequence)\n        original_word = test_target[i][1:]\n        if(original_word == decoded_word):\n            correct_count += 1\n            \n    return correct_count / test_size\n","metadata":{"execution":{"iopub.status.busy":"2022-05-07T20:50:52.869452Z","iopub.execute_input":"2022-05-07T20:50:52.869697Z","iopub.status.idle":"2022-05-07T20:50:52.896188Z","shell.execute_reply.started":"2022-05-07T20:50:52.869667Z","shell.execute_reply":"2022-05-07T20:50:52.895619Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Wandb RUN","metadata":{}},{"cell_type":"code","source":"#importing packages \nimport numpy as np\nfrom tensorflow import keras\n\n#function to run in wandb\ndef train_to_wandb():  \n    config_defaults = dict(\n            hidden_layer_size=384,\n            learning_rate=0.001,\n            optimizer='adam',\n            batch_size=512,\n            model_name = \"LSTM\",\n            embedding_size = 512,\n            dropout = 0.3,\n            epochs = 20,\n        )\n        \n    wandb.init(config = config_defaults)\n\n    wandb.run.name = \"_hls_\" + str(wandb.config.hidden_layer_size) + \"_model_\" + str(wandb.config.model_name) + \"_op_\" + str(wandb.config.optimizer) + \"_lr_\" + str(wandb.config.learning_rate) + \"_em_\" + str(wandb.config.embedding_size) + \"_dropout_\" + str(wandb.config.dropout) + \"_bs_\" + str(wandb.config.batch_size) + \"_epochs_\" + str(wandb.config.epochs)\n    \n    CONFIG = wandb.config\n\n    encoder_inputs = keras.Input(shape = (None, ))\n    encoder_outputs = keras.layers.Embedding(input_dim = num_encoder_tokens, output_dim = CONFIG.embedding_size, input_length = max_encoder_seq_length)(encoder_inputs)\n\n    # Encoder Model\n    if CONFIG.model_name == \"RNN\":\n        encoder_outputs, state = keras.layers.SimpleRNN(CONFIG.hidden_layer_size, dropout = CONFIG.dropout, return_state = True, return_sequences = True)(encoder_outputs)\n        encoder_states = [state]\n    if CONFIG.model_name == \"LSTM\":\n        encoder_outputs, state_h, state_c = keras.layers.LSTM(CONFIG.hidden_layer_size, dropout = CONFIG.dropout, return_state = True, return_sequences = True)(encoder_outputs)\n        encoder_states = [state_h,state_c]\n    if CONFIG.model_name == \"GRU\":\n        encoder_outputs, state = keras.layers.GRU(CONFIG.hidden_layer_size, dropout = CONFIG.dropout, return_state = True, return_sequences = True)(encoder_outputs)\n        encoder_states = [state]\n\n    # Decoder Model\n    decoder_inputs = keras.Input(shape=(None, ))\n    decoder_outputs = keras.layers.Embedding(input_dim = num_decoder_tokens, output_dim = CONFIG.embedding_size, input_length = max_decoder_seq_length)(decoder_inputs)\n\n    # We will test on only one layer of encoder and only one layer of decoder model\n\n    if CONFIG.model_name == \"RNN\":\n        decoder = keras.layers.SimpleRNN(CONFIG.hidden_layer_size, dropout = CONFIG.dropout, return_sequences = True, return_state = True)\n        decoder_outputs, state = decoder(decoder_outputs, initial_state = encoder_states)\n        decoder_states = [state]\n    elif CONFIG.model_name == \"LSTM\":\n        decoder = keras.layers.LSTM(CONFIG.hidden_layer_size, dropout = CONFIG.dropout, return_sequences = True, return_state = True)\n        decoder_outputs, state_h, state_c = decoder(decoder_outputs, initial_state = encoder_states)\n        decoder_states = [state_h, state_c]\n    elif CONFIG.model_name == \"GRU\":\n        decoder = keras.layers.GRU(CONFIG.hidden_layer_size, dropout = CONFIG.dropout, return_sequences = True, return_state = True)\n        decoder_outputs, state = decoder(decoder_outputs, initial_state = encoder_states)\n        decoder_states = [state]\n    else:\n        print(\"Wrong Model Choice\")\n        \n    # Adding Attention Layer\n    attention = AttentionLayer()\n    attention_output, _ = attention([encoder_outputs, decoder_outputs])\n    concatenated_decoder_input = keras.layers.Concatenate(axis = -1)([decoder_outputs, attention_output])\n\n    #Decoder Dense Layer\n    decoder_dense = keras.layers.Dense(num_decoder_tokens, activation = \"softmax\")\n    decoder_outputs = decoder_dense(concatenated_decoder_input)\n\n    #Runnable Model\n    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n    model.summary()\n    \n    #Different Optimizers\n    if CONFIG.optimizer == 'adam':\n        model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n    elif CONFIG.optimizer == 'nadam':\n        model.compile(optimizer=\"nadam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n    elif CONFIG.optimizer == 'rmsprop':\n        model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n    else:\n        print(\"Wrong optimizer Choice...\")\n        \n    #Model fitting with train and validation data characterwise\n    model.fit(\n        [train_encoder_input_data, train_decoder_input_data],\n        train_decoder_target_data,\n        batch_size = CONFIG.batch_size,\n        epochs = CONFIG.epochs,\n        validation_data = ([dev_encoder_input_data, dev_decoder_input_data], dev_decoder_target_data),\n        callbacks = [WandbCallback()]\n    )\n\n    #Wordwise Validation Data and Accuracy on the model\n    validation_accuracy = attention_inference(model,dev_encoder_input_data, dev_input, dev_target, num_decoder_tokens, max_decoder_seq_length, target_token_index, inverse_target_token_index, CONFIG.hidden_layer_size, CONFIG.model_name)\n    print(\"Attention_Wordwise_Val_Accuracy: \", validation_accuracy)\n    #Wandb Log for the Wordwise Accuracy\n    wandb.log( { \"Attention_Wordwise_Val_Accuracy\": validation_accuracy}) ","metadata":{"execution":{"iopub.status.busy":"2022-05-07T20:50:52.898351Z","iopub.execute_input":"2022-05-07T20:50:52.898695Z","iopub.status.idle":"2022-05-07T20:50:52.928660Z","shell.execute_reply.started":"2022-05-07T20:50:52.898666Z","shell.execute_reply":"2022-05-07T20:50:52.928038Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Sweeps Config","metadata":{}},{"cell_type":"code","source":"sweep_config = {\n  \"name\": \"Attention Bayesian Sweep 1\",\n  \"method\": \"bayes\",\n  \"metric\":{\n  \"name\": \"Attention_Wordwise_Val_Accuracy\",\n  \"goal\": \"maximize\"\n  },\n  \"parameters\": {\n        \"hidden_layer_size\": {\n            \"values\": [128,256]\n        },\n\n        \"learning_rate\": {\n            \"values\": [0.001]\n        },\n        \"optimizer\": {\n            \"values\": ['adam','nadam']\n        },\n        \n        \"batch_size\": {\n            \"values\": [256, 512]\n        },\n        \n        \"model_name\": {\n            \"values\": [\"LSTM\"]\n        },\n \n        \"embedding_size\": {\n            \"values\": [256, 512]\n        },\n        \n        \"dropout\": {\n            \"values\": [0.3]\n        },\n                    \n        \"epochs\": {\n            \"values\": [15,20,25]\n        },\n      \n        \n        \n    }\n}\n","metadata":{"execution":{"iopub.status.busy":"2022-05-07T20:50:52.929838Z","iopub.execute_input":"2022-05-07T20:50:52.930400Z","iopub.status.idle":"2022-05-07T20:50:52.943182Z","shell.execute_reply.started":"2022-05-07T20:50:52.930369Z","shell.execute_reply":"2022-05-07T20:50:52.942190Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Importing Wandb Packages","metadata":{}},{"cell_type":"code","source":"import wandb\nfrom tensorflow.keras.optimizers import Adam, RMSprop, Nadam\nfrom wandb.keras import WandbCallback","metadata":{"execution":{"iopub.status.busy":"2022-05-07T20:50:52.944320Z","iopub.execute_input":"2022-05-07T20:50:52.944559Z","iopub.status.idle":"2022-05-07T20:50:53.774204Z","shell.execute_reply.started":"2022-05-07T20:50:52.944525Z","shell.execute_reply":"2022-05-07T20:50:53.773281Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Calling Sweeps","metadata":{}},{"cell_type":"code","source":"sweep_id = wandb.sweep(sweep_config,project=\"CS6910-Assignment-3\", entity=\"cs21m024_cs21m075\")\nwandb.agent(sweep_id, train_to_wandb , count = 20)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T20:50:53.775556Z","iopub.execute_input":"2022-05-07T20:50:53.775774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}