{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Vanilla Mechanism Wandb Run Code","metadata":{"_uuid":"7beb49a4-fc8c-45bb-835e-95ffceabd252","_cell_guid":"6fdddce6-cb0b-4956-8a86-153b5ad3e69f","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-07T10:06:25.915388Z","iopub.execute_input":"2022-05-07T10:06:25.915856Z","iopub.status.idle":"2022-05-07T10:06:31.531173Z","shell.execute_reply.started":"2022-05-07T10:06:25.915767Z","shell.execute_reply":"2022-05-07T10:06:31.530269Z"}}},{"cell_type":"markdown","source":"**Submitted By:**\nJoyojyoti Acharya - CS21M024,\nVrushab Karia - CS21M075","metadata":{}},{"cell_type":"markdown","source":"### Importing the Necessary Packages","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tensorflow import keras\nimport tensorflow as tf\ntf.test.gpu_device_name()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T13:05:50.473037Z","iopub.execute_input":"2022-05-07T13:05:50.473351Z","iopub.status.idle":"2022-05-07T13:05:56.950452Z","shell.execute_reply.started":"2022-05-07T13:05:50.473271Z","shell.execute_reply":"2022-05-07T13:05:56.949760Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Importing Dataset","metadata":{}},{"cell_type":"code","source":"#Using target Language as Hindi\ntarget_language = \"hi\"\nDATAPATH = \"/kaggle/input/dakshina/dakshina_dataset_v1.0/{}/lexicons/{}.translit.sampled.{}.tsv\"\n\n#Defining training, validation and test path and reading the data from dataset.\n\n#Training\ntrain_path = DATAPATH.format(target_language, target_language, \"train\")\ntrain_data = pd.read_csv(train_path, sep = '\\t', header = None)\n\n#Validation\ndev_path = DATAPATH.format(target_language, target_language, \"dev\")\ndev_data = pd.read_csv(dev_path, sep = '\\t', header = None)\n\n#Test\ntest_path = DATAPATH.format(target_language, target_language, \"test\")\ntest_data = pd.read_csv(test_path, sep = '\\t', header = None)","metadata":{"_uuid":"845aa7e0-d192-4dc0-9f51-581945c7aaa6","_cell_guid":"5486dcde-0f75-426a-89de-ba65db3b8a9c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-07T13:05:58.572079Z","iopub.execute_input":"2022-05-07T13:05:58.572710Z","iopub.status.idle":"2022-05-07T13:05:58.674701Z","shell.execute_reply.started":"2022-05-07T13:05:58.572675Z","shell.execute_reply":"2022-05-07T13:05:58.673984Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Spliting the dataset into wordwise and characterwise","metadata":{}},{"cell_type":"code","source":"#All unique characters\ninput_characters = set()\ntarget_characters = set()\ninput_characters.add(' ')\ntarget_characters.add(' ')\n\n#Training Data\ntrain_input = [str(w) for w in train_data[1]]\ntrain_target = [\"\\t\" + str(w) + \"\\n\" for w in train_data[0]]\nfor word in train_input:\n    for char in word:\n        input_characters.add(char)\nfor word in train_target:\n    for char in word:\n        target_characters.add(char)\n\n#Validation Data\ndev_input = [str(w) for w in dev_data[1]]\ndev_target = [\"\\t\" + str(w) + \"\\n\" for w in dev_data[0]]\nfor word in dev_input:\n    for char in word:\n        input_characters.add(char)\nfor word in dev_target:\n    for char in word:\n        target_characters.add(char)\n\n#Test Data\ntest_input = [str(w) for w in test_data[1]]\ntest_target = [\"\\t\" + str(w) + \"\\n\" for w in test_data[0]]\n\nfor word in test_input:\n    for char in word:\n        input_characters.add(char) \nfor word in test_target:\n    for char in word:\n        target_characters.add(char)\n        \n#Sorting the characters\ninput_characters = list(input_characters)\ntarget_characters = list(target_characters)\ninput_characters.sort()\ntarget_characters.sort()","metadata":{"_uuid":"1959ce74-ca5f-4234-9efd-5442301b620b","_cell_guid":"51827efb-4d98-479e-b627-16f80867feb8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-07T13:06:01.065084Z","iopub.execute_input":"2022-05-07T13:06:01.065374Z","iopub.status.idle":"2022-05-07T13:06:01.256316Z","shell.execute_reply.started":"2022-05-07T13:06:01.065341Z","shell.execute_reply":"2022-05-07T13:06:01.255554Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Fetching character and maximum sequence length","metadata":{}},{"cell_type":"code","source":"num_encoder_tokens = len(input_characters)\nnum_decoder_tokens = len(target_characters)\nmax_encoder_seq_length = max(max([len(text) for text in train_input]),max([len(text) for text in dev_input]))\nmax_encoder_seq_length = max(max_encoder_seq_length,max([len(text) for text in test_input]))\n                             \nmax_decoder_seq_length = max(max([len(text) for text in train_target]),max([len(text) for text in dev_target]))\nmax_decoder_seq_length = max(max_decoder_seq_length,max([len(text) for text in test_target]))\n                             \nprint(\"Number of Training samples:\", len(train_input))\nprint(\"Number of Validation samples:\", len(dev_input))\nprint(\"Number of Test samples:\", len(test_input))\n                             \nprint(\"Number of unique input tokens:\", num_encoder_tokens)\nprint(\"Number of unique output tokens:\", num_decoder_tokens)\nprint(\"Max sequence length for inputs:\", max_encoder_seq_length)\nprint(\"Max sequence length for outputs:\", max_decoder_seq_length)","metadata":{"_uuid":"3efca845-cc19-4c29-8343-9c4b3275d46c","_cell_guid":"fe883fc7-f715-4ae3-9683-88f9244a64d0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-07T13:06:01.745987Z","iopub.execute_input":"2022-05-07T13:06:01.746237Z","iopub.status.idle":"2022-05-07T13:06:01.766305Z","shell.execute_reply.started":"2022-05-07T13:06:01.746210Z","shell.execute_reply":"2022-05-07T13:06:01.765459Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Dictionary Indexing and Inverse Dictionary Indexing for the unique Characters","metadata":{}},{"cell_type":"code","source":"input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\ninverse_input_token_index = dict([(i, char) for i, char in enumerate(input_characters)])\ntarget_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\ninverse_target_token_index = dict([(i, char) for i, char in enumerate(target_characters)])","metadata":{"_uuid":"b05c0b87-4b2b-4ad9-8f78-054e9d08860f","_cell_guid":"28d8c813-0259-4a8a-b378-fdeae4616e1f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-07T13:06:03.976880Z","iopub.execute_input":"2022-05-07T13:06:03.977124Z","iopub.status.idle":"2022-05-07T13:06:03.985752Z","shell.execute_reply.started":"2022-05-07T13:06:03.977098Z","shell.execute_reply":"2022-05-07T13:06:03.984761Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Training Encoder-Decoder One Hot Data Preparation","metadata":{}},{"cell_type":"code","source":"train_encoder_input_data = np.zeros((len(train_input), max_encoder_seq_length), dtype=\"float32\")\ntrain_decoder_input_data = np.zeros((len(train_input), max_decoder_seq_length), dtype=\"float32\")\ntrain_decoder_target_data = np.zeros((len(train_input), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\nfor i, (input_text, target_text) in enumerate(zip(train_input, train_target)):\n    for t, char in enumerate(input_text):\n        train_encoder_input_data[i, t] = input_token_index[char]\n    train_encoder_input_data[i, t + 1 :] = input_token_index[' ']\n    for t, char in enumerate(target_text):\n        train_decoder_input_data[i, t] = target_token_index[char]\n        if t > 0:\n            train_decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n    train_decoder_input_data[i, t + 1 :] = target_token_index[' ']\n    train_decoder_target_data[i, t:, target_token_index[' ']] =  1.0","metadata":{"_uuid":"e1adb64d-9e3a-4b2d-a2c4-3d8011389151","_cell_guid":"f8f58c25-bd50-4a65-b3f3-ae77728f571c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-07T13:06:05.214901Z","iopub.execute_input":"2022-05-07T13:06:05.215149Z","iopub.status.idle":"2022-05-07T13:06:05.975950Z","shell.execute_reply.started":"2022-05-07T13:06:05.215123Z","shell.execute_reply":"2022-05-07T13:06:05.975229Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Validation Encoder-Decoder One Hot Data Preparation","metadata":{}},{"cell_type":"code","source":"dev_encoder_input_data = np.zeros((len(dev_input), max_encoder_seq_length), dtype=\"float32\")\ndev_decoder_input_data = np.zeros((len(dev_input), max_decoder_seq_length), dtype=\"float32\")\ndev_decoder_target_data = np.zeros((len(dev_input), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\nfor i, (input_text, target_text) in enumerate(zip(dev_input, dev_target)):\n    for t, char in enumerate(input_text):\n        dev_encoder_input_data[i, t] = input_token_index[char]\n    dev_encoder_input_data[i, t + 1 :] = input_token_index[' ']\n    for t, char in enumerate(target_text):\n        dev_decoder_input_data[i, t] = target_token_index[char]\n        if t > 0:\n            dev_decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n    dev_decoder_input_data[i, t + 1 :] = target_token_index[' ']\n    dev_decoder_target_data[i, t:, target_token_index[' '] ] = 1.0","metadata":{"_uuid":"23c10639-0c7c-4674-ac5c-b16bd5e52df6","_cell_guid":"214790a4-ad12-43e5-8f5a-012ac7d7df81","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-07T13:06:05.977584Z","iopub.execute_input":"2022-05-07T13:06:05.977986Z","iopub.status.idle":"2022-05-07T13:06:06.061196Z","shell.execute_reply.started":"2022-05-07T13:06:05.977948Z","shell.execute_reply":"2022-05-07T13:06:06.060510Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Wordwise Inference Mechanism for Vanilla Approach","metadata":{}},{"cell_type":"code","source":"#Sigmoid Function\ndef sigmoid(i):\n    return [1/(1 + np.exp(-z)) for z in i]\n\n# Vanilla_Inference Function\ndef vanilla_inference(model, dev_encoder_input_data, dev_input, dev_target, num_decoder_tokens, max_decoder_seq_length, target_token_index, inverse_target_token_index, encoder_latent_dim, decoder_latent_dim, model_name):\n    \n    #Prediction Function --> Wordwise\n    def decode_sequence_predict(input_sequence):\n        # Encode the input as state vectors.\n        states_value = [encoder_model.predict(input_sequence)] * len(decoder_models_index)\n        # Generate empty target sequence of length 1.\n        target_sequence = np.zeros(( 1, 1))\n\n        # Populate the first character of target sequence with the start character.\n        target_sequence[0, 0 ] = target_token_index[\"\\t\"]\n        \n        flag = True\n        output_sequence = \"\"\n\n        while flag:\n            output = decoder_model.predict([target_sequence] + states_value)\n            output_tokens, states_value = output[0], output[1:]\n\n            # Sample a token\n            sample_token_index = np.argmax(output_tokens[0, -1, :])\n            sample_chararcter = inverse_target_token_index[sample_token_index]\n            output_sequence += sample_chararcter\n            if sample_chararcter == \"\\n\" or len(output_sequence) > max_decoder_seq_length:\n                flag = False\n            target_sequence = np.zeros((1, 1))\n            target_sequence[0, 0] = sample_token_index \n        return output_sequence\n    \n    \n    no_of_encoder_layers = len(encoder_latent_dim)\n    encoder_embedding_index, encoder_models_index = -1, []\n    decoder_embedding_index, decoder_models_index = -1, []\n    dense_index = -1\n    encoder_layers_count = 0\n    \n    flag = True\n    for idx, layer in enumerate(model.layers):\n        print(layer.name)\n        # Dense Layer\n        if \"dense\" in layer.name :\n            dense_index = idx\n\n        # Embedding layer\n        if \"embedding\" in layer.name:\n            if flag:\n                encoder_embedding_index = idx\n                flag = False\n            else:\n                decoder_embedding_index = idx\n\n        # Encoder-Decoder Model Layers \n        if model_name.lower() in layer.name:\n            if encoder_layers_count < no_of_encoder_layers:\n                encoder_models_index.append(idx)\n                encoder_layers_count += 1\n            else:\n                decoder_models_index.append(idx)\n\n    \n    # Encoder Model\n    encoder_inputs = model.input[0]  # input_1\n\n    if model_name == \"RNN\" or model_name == \"GRU\":\n        encoder_outputs, state = model.layers[encoder_models_index[-1]].output\n        encoder_model = keras.Model(encoder_inputs, [state])\n    \n    elif model_name == \"LSTM\":\n        encoder_outputs, state_h_enc, state_c_enc = model.layers[encoder_models_index[-1]].output\n        encoder_model = keras.Model(encoder_inputs, [state_h_enc, state_c_enc])\n    \n    else:\n        print(\"Wrong Choice of Model...\")\n        return\n\n    #Decoder Model\n    decoder_inputs = model.input[1]  # input_2\n    decoder_outputs =  model.layers[decoder_embedding_index](decoder_inputs)\n\n    decoder_states_inputs =  []\n    decoder_states = []\n\n    # Decoder Models\n    for dec in range(len(decoder_latent_dim)):\n        \n        if model_name == \"RNN\" or model_name == \"GRU\":\n            state = keras.Input(shape = (decoder_latent_dim[dec], ))\n            current_states_inputs = [state]\n            decoder_outputs, state = model.layers[decoder_models_index[dec]](decoder_outputs, initial_state = current_states_inputs)\n            decoder_states += [state]\n\n        elif model_name == \"LSTM\":\n            state_h_dec, state_c_dec = keras.Input(shape = (decoder_latent_dim[dec],)),  keras.Input(shape = (decoder_latent_dim[dec],))\n            current_states_inputs = [state_h_dec, state_c_dec]\n            decoder_outputs, state_h_dec,state_c_dec = model.layers[decoder_models_index[dec]](decoder_outputs, initial_state = current_states_inputs)\n            decoder_states += [state_h_dec, state_c_dec]\n            \n        else:\n            print(\"Wrong Choice of Model...\")\n        \n        decoder_states_inputs += current_states_inputs\n\n    # Decoder Dense layer\n    decoder_dense = model.layers[dense_index]\n    decoder_outputs = decoder_dense(decoder_outputs)\n\n    # Final decoder model\n    decoder_model = keras.Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n\n    #Count of correct Predictions\n    correct_count = 0\n    for idx in range(len(dev_input)):\n        if idx%50 == 0:\n            print(\"Test at: \", idx)\n        input_sequence = dev_encoder_input_data[idx : idx + 1]\n        decoded_word = decode_sequence_predict(input_sequence)\n        original_word = dev_target[idx][1:]\n        if(original_word == decoded_word):\n            correct_count += 1\n\n    return correct_count/len(dev_input)","metadata":{"_uuid":"54d6a9f2-b934-46a4-9575-ac1a0a95dd25","_cell_guid":"ce55a66e-d5be-4f75-b68d-4de19ba28dfe","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-07T13:06:06.224659Z","iopub.execute_input":"2022-05-07T13:06:06.225172Z","iopub.status.idle":"2022-05-07T13:06:06.255167Z","shell.execute_reply.started":"2022-05-07T13:06:06.225135Z","shell.execute_reply":"2022-05-07T13:06:06.254270Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Wandb RUN","metadata":{}},{"cell_type":"code","source":"#function to run in wandb \ndef train_to_wandb():  \n    config_defaults = dict(\n            hidden_layer_size=512,\n            num_encoder_layers=3,\n            num_decoder_layers=3,\n            learning_rate=0.001,\n            optimizer='nadam',\n            batch_size=64,\n            model_name = \"LSTM\",\n            embedding_size = 512,\n            dropout = 0.3,\n            epochs = 25,\n            beam_size = 0,\n        )\n        \n    wandb.init(config = config_defaults)\n\n    wandb.run.name = \"el_\" + str(wandb.config.num_encoder_layers) + \"_dl_\" + str(wandb.config.num_decoder_layers) + \"_hls_\" + str(wandb.config.hidden_layer_size) + \"_model_\" + str(wandb.config.model_name) + \"_op_\" + str(wandb.config.optimizer) + \"_lr_\" + str(wandb.config.learning_rate) + \"_em_\" + str(wandb.config.embedding_size) + \"_dropout_\" + str(wandb.config.dropout) + \"_bs_\" + str(wandb.config.batch_size) + \"_epochs_\" + str(wandb.config.epochs) + \"_bm_\" + str(wandb.config.beam_size)\n    \n    CONFIG = wandb.config\n\n    \n    #Encoder_Model\n    encoder_inputs = keras.Input(shape=(None, ))\n    encoder_outputs = keras.layers.Embedding(input_dim = num_encoder_tokens,\n                                             output_dim = CONFIG.embedding_size,\n                                             input_length = max_encoder_seq_length)(encoder_inputs)\n    \n    encoder_latent_dim = [CONFIG.hidden_layer_size]*CONFIG.num_encoder_layers\n    for latent_dim in encoder_latent_dim:\n        if CONFIG.model_name == \"RNN\":\n            encoder_outputs, state = keras.layers.SimpleRNN(latent_dim, dropout = CONFIG.dropout, return_state = True, return_sequences = True)(encoder_outputs)\n            encoder_states = [state]\n        elif CONFIG.model_name == \"LSTM\":\n            encoder_outputs, state_h, state_c = keras.layers.LSTM(latent_dim, dropout = CONFIG.dropout, return_state = True, return_sequences = True)(encoder_outputs)\n            encoder_states = [state_h, state_c]\n        elif CONFIG.model_name == \"GRU\":\n            encoder_outputs, state = keras.layers.GRU(latent_dim, dropout = CONFIG.dropout, return_state = True, return_sequences = True)(encoder_outputs)\n            encoder_states = [state]\n        else:\n            print(\"Wrong Model Choice\")\n\n    #Decoder Model\n    decoder_inputs = keras.Input(shape=(None, ))\n    decoder_outputs = keras.layers.Embedding(input_dim = num_decoder_tokens,\n                                             output_dim = CONFIG.embedding_size,\n                                             input_length = max_decoder_seq_length)(decoder_inputs)\n    \n    decoder_latent_dim = [CONFIG.hidden_layer_size]*CONFIG.num_decoder_layers\n    for latent_dim in decoder_latent_dim:\n        if CONFIG.model_name == \"RNN\":\n            decoder = keras.layers.SimpleRNN(latent_dim, dropout = CONFIG.dropout, return_sequences = True, return_state = True)\n            decoder_outputs, _ = decoder(decoder_outputs, initial_state = encoder_states)\n        elif CONFIG.model_name == \"LSTM\":\n            decoder = keras.layers.LSTM(latent_dim, dropout = CONFIG.dropout, return_sequences = True, return_state = True)\n            decoder_outputs, _, _ = decoder(decoder_outputs, initial_state = encoder_states)\n        elif CONFIG.model_name == \"GRU\":\n            decoder = keras.layers.GRU(latent_dim, dropout = CONFIG.dropout, return_sequences = True, return_state = True)\n            decoder_outputs, _= decoder(decoder_outputs, initial_state = encoder_states)\n        else:\n            print(\"Wrong Model Choice\")\n\n    #Decoder Dense Layer\n    decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n    decoder_outputs = decoder_dense(decoder_outputs)\n    \n    #Runnable Model\n    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n    model.summary()\n\n    #Different Optimizers\n    if CONFIG.optimizer == 'adam':\n        model.compile(optimizer = Adam(learning_rate=CONFIG.learning_rate), loss=\"categorical_crossentropy\", metrics = ['accuracy'])\n    elif CONFIG.optimizer == 'nadam':\n        model.compile(optimizer = Nadam(learning_rate=CONFIG.learning_rate), loss=\"categorical_crossentropy\", metrics = ['accuracy'])\n    elif CONFIG.optimizer == 'rmsprop':\n        model.compile(optimizer = RMSprop(learning_rate=CONFIG.learning_rate), loss=\"categorical_crossentropy\", metrics = ['accuracy'])\n    \n    #Model fitting with train and validation data characterwise\n    model.fit(\n        [train_encoder_input_data, train_decoder_input_data],\n        train_decoder_target_data,\n        batch_size=CONFIG.batch_size,\n        epochs=CONFIG.epochs,\n        validation_data = ([dev_encoder_input_data, dev_decoder_input_data], dev_decoder_target_data),\n        callbacks = [WandbCallback()]\n    )\n    \n    #Wordwise Validation Data and Accuracy on the model\n    validation_accuracy = vanilla_inference(model, dev_encoder_input_data, dev_input, dev_target, num_decoder_tokens, max_decoder_seq_length, target_token_index, inverse_target_token_index, encoder_latent_dim, decoder_latent_dim, CONFIG.model_name)\n    print(\"Wordlevel Validation Accuracy: \", validation_accuracy)\n    #Wandb Log for the Wordwise Accuracy\n    wandb.log( { \"WordLevel_Validation_Accuracy\": validation_accuracy})","metadata":{"_uuid":"597e44f7-8add-4e8d-ac3a-f11473ffe284","_cell_guid":"e3da53b4-f9ea-4142-bd60-91d1e22d8c3e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-07T13:06:07.543664Z","iopub.execute_input":"2022-05-07T13:06:07.543920Z","iopub.status.idle":"2022-05-07T13:06:07.567333Z","shell.execute_reply.started":"2022-05-07T13:06:07.543891Z","shell.execute_reply":"2022-05-07T13:06:07.565885Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Sweeps Config","metadata":{}},{"cell_type":"code","source":"sweep_config = {\n  \"name\": \"Bayesian Sweep\",\n  \"method\": \"bayes\",\n  \"metric\":{\n  \"name\": \"WordLevel_Validation_Accuracy\",\n  \"goal\": \"maximize\"\n  },\n  \"parameters\": {\n        \"hidden_layer_size\": {\n            \"values\": [128,256,512]\n        },\n        \"num_encoder_layers\": {\n            \"values\": [1,2,3]\n        },\n         \"num_decoder_layers\": {\n            \"values\": [1,2,3]\n        },\n        \"learning_rate\": {\n            \"values\": [0.001,0.0001]\n        },\n        \"optimizer\": {\n            \"values\": ['adam','rmsprop','nadam']\n        },\n        \n        \"batch_size\": {\n            \"values\": [64,128,256]\n        },\n        \n        \"model_name\": {\n            \"values\": [\"RNN\",\"GRU\",\"LSTM\"]\n        },\n \n        \"embedding_size\": {\n            \"values\": [128,256,512]\n        },\n        \n        \"dropout\": {\n            \"values\": [0.1,0.2,0.3]\n        },\n                    \n        \"epochs\": {\n            \"values\": [20,25]\n        },\n      \n        \"beam_size\": {\n            \"values\": [0]\n        },\n        \n        \n    }\n}","metadata":{"_uuid":"3670c4d2-49f0-4c07-ac0e-1ce3a8aebaec","_cell_guid":"f928b51f-347f-4089-a60e-5c333c4ad4fe","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-07T13:06:10.743788Z","iopub.execute_input":"2022-05-07T13:06:10.744314Z","iopub.status.idle":"2022-05-07T13:06:10.752546Z","shell.execute_reply.started":"2022-05-07T13:06:10.744278Z","shell.execute_reply":"2022-05-07T13:06:10.751562Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Importing Wandb Packages","metadata":{}},{"cell_type":"code","source":"import wandb\nfrom tensorflow.keras.optimizers import Adam, RMSprop, Nadam\nfrom wandb.keras import WandbCallback","metadata":{"_uuid":"bf363c30-6b82-489d-97aa-61c4f2402676","_cell_guid":"6c942741-6755-4a64-a04f-12278d8cee9f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-07T13:06:12.521348Z","iopub.execute_input":"2022-05-07T13:06:12.521877Z","iopub.status.idle":"2022-05-07T13:06:13.956606Z","shell.execute_reply.started":"2022-05-07T13:06:12.521842Z","shell.execute_reply":"2022-05-07T13:06:13.955744Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Calling Sweeps","metadata":{}},{"cell_type":"code","source":"sweep_id = wandb.sweep(sweep_config,project=\"CS6910-Assignment-3\", entity=\"cs21m024_cs21m075\")\nwandb.agent(sweep_id, train_to_wandb , count = 20)","metadata":{"_uuid":"37b0f407-4e1b-45e9-b8c0-686b3ea8cda2","_cell_guid":"a7ab0190-e53a-45ce-8c38-721f4676a5c2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-07T13:06:14.894313Z","iopub.execute_input":"2022-05-07T13:06:14.894588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}